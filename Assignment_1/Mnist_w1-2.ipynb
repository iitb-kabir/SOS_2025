{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c5ce5c",
   "metadata": {},
   "source": [
    "## Loaded all the useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0afbfbc",
   "metadata": {},
   "source": [
    "## Loaded Dataset and Preprocessing  step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.values.astype(np.float32).reshape(-1, 1, 28, 28) / 255.0\n",
    "y = mnist.target.astype(int).values\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa4dfd",
   "metadata": {},
   "source": [
    "## Build the CNN Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe840a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.k = kernel_size\n",
    "        self.W = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n",
    "        self.b = np.zeros((out_channels, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        F, _, k, _ = self.W.shape\n",
    "        out_h = (H + 2 * self.padding - k) // self.stride + 1\n",
    "        out_w = (W + 2 * self.padding - k) // self.stride + 1\n",
    "        out = np.zeros((N, F, out_h, out_w))\n",
    "\n",
    "        x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for f in range(F):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = x_padded[n, :, h_start:h_start+k, w_start:w_start+k]\n",
    "                        out[n, f, i, j] = np.sum(region * self.W[f]) + self.b[f]\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71945b",
   "metadata": {},
   "source": [
    "## ReLU Activation and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "\n",
    "class MaxPool2D:\n",
    "    def __init__(self, size, stride):\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H - self.size) // self.stride + 1\n",
    "        out_w = (W - self.size) // self.stride + 1\n",
    "        out = np.zeros((N, C, out_h, out_w))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(out_h):\n",
    "                    for j in range(out_w):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = x[n, c, h_start:h_start+self.size, w_start:w_start+self.size]\n",
    "                        out[n, c, i, j] = np.max(region)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee797d30",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0faf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.W = np.random.randn(in_features, out_features) * 0.1\n",
    "        self.b = np.zeros((1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56f287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def forward(self, logits, labels):\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        self.probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.labels = labels\n",
    "        loss = -np.sum(labels * np.log(self.probs + 1e-9)) / logits.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return (self.probs - self.labels) / self.labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9064b",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c478b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(1, 8, 3, stride=1, padding=1)\n",
    "relu = ReLU()\n",
    "pool = MaxPool2D(2, 2)\n",
    "flatten = Flatten()\n",
    "dense = Dense(14*14*8, 10)\n",
    "loss_fn = SoftmaxCrossEntropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c55c59",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffcefaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nasir\\AppData\\Local\\Temp\\ipykernel_15080\\1225869148.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  out[n, f, i, j] = np.sum(region * self.W[f]) + self.b[f]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (first batch): 2.3387095988836353\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(x):\n",
    "    out = conv.forward(x)\n",
    "    out = relu.forward(out)\n",
    "    out = pool.forward(out)\n",
    "    out = flatten.forward(out)\n",
    "    out = dense.forward(out)\n",
    "    return out\n",
    "\n",
    "# Run one forward pass for demo\n",
    "logits = forward_pass(X_train[:32])\n",
    "loss = loss_fn.forward(logits, y_train[:32])\n",
    "print(f\"Loss (first batch): {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9ef26",
   "metadata": {},
   "source": [
    "## Applying ConvNet by using Pytroch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b54b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 8, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(14*14*8, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 14*14*8)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b953f9",
   "metadata": {},
   "source": [
    "## Accuracy by using Numpy CNN without any training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1172da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nasir\\AppData\\Local\\Temp\\ipykernel_15080\\1225869148.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  out[n, f, i, j] = np.sum(region * self.W[f]) + self.b[f]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy CNN Test Accuracy: 10.335714285714285\n"
     ]
    }
   ],
   "source": [
    "def accuracy(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    targets = np.argmax(labels, axis=1)\n",
    "    return np.mean(preds == targets)\n",
    "\n",
    "test_logits = forward_pass(X_test)\n",
    "test_acc = accuracy(test_logits, y_test)\n",
    "print(\"NumPy CNN Test Accuracy:\", test_acc * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96bd54",
   "metadata": {},
   "source": [
    "## Accuracy by using Pytorch CNN without any training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37479a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CNN Test Accuracy: 10.285714285714286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(\"PyTorch CNN Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7ac29",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "The comparison below demonstrates the behavior of a Convolutional Neural Network (CNN) implemented using both **NumPy** and **PyTorch**, evaluated **without any training**.\n",
    "\n",
    "## Test Accuracy Results\n",
    "\n",
    "- **NumPy CNN Accuracy:** 10.33%\n",
    "- **PyTorch CNN Accuracy:** 10.28%\n",
    "\n",
    "The minor difference in accuracy (~0.05%) is negligible. Both models have randomly initialized weights, so for a 10-class classification task, the expected accuracy due to random guessing is close to **10%**.\n",
    "\n",
    "## Execution Time Comparison\n",
    "\n",
    "- **NumPy:** ~12 minutes and 50 seconds  \n",
    "- **PyTorch:** 1.1 seconds  \n",
    "\n",
    "This significant difference highlights a major advantage of PyTorch:\n",
    "\n",
    "> **PyTorch is highly optimized for performance and can leverage GPU acceleration, whereas NumPy is restricted to CPU-based operations and lacks deep learning-specific optimizations.**\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Without training, both CNNs perform close to chance level (≈10%).\n",
    "- PyTorch is **much faster** during evaluation and suitable for real-world, large-scale problems.\n",
    "- NumPy is ideal for educational purposes and understanding the inner workings of CNNs.\n",
    "- For practical deployment, **PyTorch is preferred** due to its performance, scalability, and support for hardware acceleration.\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Framework | Accuracy (%) | Evaluation Time |\n",
    "|-----------|---------------|-----------------|\n",
    "| NumPy     | 10.33         | 12m 50s         |\n",
    "| PyTorch   | 10.28         | 1.1s            |\n",
    "\n",
    "*Table: Comparison of NumPy and PyTorch CNN evaluation without training*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328cc30a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
